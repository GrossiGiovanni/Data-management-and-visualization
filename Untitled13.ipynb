{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM3w2dssdoYqtyQxopC9eqI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GrossiGiovanni/Data-management-and-visualization/blob/main/Untitled13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZfOLjXCUyB1",
        "outputId": "9183dc5c-753a-4d57-b8ae-742764665f04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (23.1)\n",
            "Requirement already satisfied: protobuf<4,>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorboardX"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oJT8g7IU1m9",
        "outputId": "f47dd159-b2a7-43b0-961f-da041678a715"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "import shutil\n",
        "from config import cfg\n",
        "\n",
        "def weights_init_kaiming(m):\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        #kaiming is first name of author whose last name is 'He' lol\n",
        "        nn.init.kaiming_uniform(m.weight) \n",
        "        m.bias.data.zero_()\n",
        "\n",
        "def adjust_learning_rate(lr, decay, optimizer, cur_epoch, n_epochs):\n",
        "    \"\"\"Sets the learning rate to the initially \n",
        "        configured `lr` decayed by `decay` every `n_epochs`\"\"\"\n",
        "    new_lr = lr * (decay ** (cur_epoch // n_epochs))\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = new_lr\n",
        "\n",
        "def calculate_mean_iu(predictions, gts, num_classes):\n",
        "    sum_iu = 0\n",
        "    for i in range(num_classes):\n",
        "        n_ii = t_i = sum_n_ji = 1e-9\n",
        "        for p, gt in zip(predictions, gts):\n",
        "            n_ii += np.sum(gt[p == i] == i)\n",
        "            t_i += np.sum(gt == i)\n",
        "            sum_n_ji += np.sum(p == i)\n",
        "        sum_iu += float(n_ii) / (t_i + sum_n_ji - n_ii)\n",
        "    mean_iu = sum_iu / num_classes\n",
        "    return mean_iu\n",
        "\n",
        "class CrossEntropyLoss2d(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        super(CrossEntropyLoss2d, self).__init__()\n",
        "        self.nll_loss = nn.NLLLoss2d(weight, size_average)\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        return self.nll_loss(F.log_softmax(inputs), targets)\n",
        "\n",
        "def rmrf_mkdir(dir_name):\n",
        "    if os.path.exists(dir_name):\n",
        "        shutil.rmtree(dir_name)\n",
        "    os.mkdir(dir_name)\n",
        "\n",
        "def rm_file(path_file):\n",
        "    if os.path.exists(path_file):\n",
        "        os.remove(path_file)\n",
        "\n",
        "def colorize_mask(mask):\n",
        "    # mask: numpy array of the mask\n",
        "    new_mask = Image.fromarray(mask.astype(np.uint8)).convert('P')\n",
        "    new_mask.putpalette(cfg.VIS.PALETTE_LABEL_COLORS)\n",
        "\n",
        "    return new_mask\n",
        "\n",
        "#============================\n",
        "\n",
        "\n",
        "def _fast_hist(label_true, label_pred, n_class):\n",
        "    mask = (label_true >= 0) & (label_true < n_class)\n",
        "    hist = np.bincount(\n",
        "        n_class * label_true[mask].astype(int) +\n",
        "        label_pred[mask], minlength=n_class**2).reshape(n_class, n_class)\n",
        "    return hist\n",
        "\n",
        "\n",
        "def scores(label_trues, label_preds, n_class):\n",
        "    \"\"\"Returns accuracy score evaluation result.\n",
        "      - overall accuracy\n",
        "      - mean accuracy\n",
        "      - mean IU\n",
        "      - fwavacc\n",
        "    \"\"\"\n",
        "    hist = np.zeros((n_class, n_class))\n",
        "    for lt, lp in zip(label_trues, label_preds):\n",
        "        hist += _fast_hist(lt.flatten(), lp.flatten(), n_class)\n",
        "    acc = np.diag(hist).sum() / hist.sum()\n",
        "    acc_cls = np.diag(hist) / hist.sum(axis=1)\n",
        "    acc_cls = np.nanmean(acc_cls)\n",
        "    iu = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist))\n",
        "    mean_iu = np.nanmean(iu)\n",
        "    freq = hist.sum(axis=1) / hist.sum()\n",
        "    fwavacc = (freq[freq > 0] * iu[freq > 0]).sum()\n",
        "    cls_iu = dict(zip(range(n_class), iu))\n",
        "\n",
        "    return {'Overall Acc: \\t': acc,\n",
        "            'Mean Acc : \\t': acc_cls,\n",
        "            'FreqW Acc : \\t': fwavacc,\n",
        "            'Mean IoU : \\t': mean_iu,}, cls_iu\n",
        "            \n",
        "def calculate_iu_per_class(predictions, gts, num_classes):\n",
        "    iu_list = []\n",
        "    for i in range(num_classes):\n",
        "        n_ii = t_i = sum_n_ji = 1e-9\n",
        "        for p, gt in zip(predictions, gts):\n",
        "            n_ii += np.sum(gt[p == i] == i)\n",
        "            t_i += np.sum(gt == i)\n",
        "            sum_n_ji += np.sum(p == i)\n",
        "        iu = float(n_ii) / (t_i + sum_n_ji - n_ii)\n",
        "        iu_list.append(iu)\n",
        "    return iu_list\n"
      ],
      "metadata": {
        "id": "YAaHb2R4fjiv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import torch\n",
        "from torch import optim\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import NLLLoss2d\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torchvision.utils import save_image\n",
        "import torchvision.transforms as standard_transforms\n",
        "import torchvision.utils as vutils\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "from model import ENet\n",
        "from config import cfg\n",
        "from loading_data import loading_data\n",
        "from utils import *\n",
        "from timer import Timer\n",
        "import pdb\n",
        "\n",
        "exp_name = cfg.TRAIN.EXP_NAME\n",
        "log_txt = cfg.TRAIN.EXP_LOG_PATH + '/' + exp_name + '.txt'\n",
        "writer = SummaryWriter(cfg.TRAIN.EXP_PATH+ '/' + exp_name)\n",
        "\n",
        "pil_to_tensor = standard_transforms.ToTensor()\n",
        "train_loader, val_loader, restore_transform = loading_data()\n",
        "\n",
        "def main():\n",
        "\n",
        "    cfg_file = open('./config.py',\"r\")  \n",
        "    cfg_lines = cfg_file.readlines()\n",
        "    \n",
        "    with open(log_txt, 'a') as f:\n",
        "            f.write(''.join(cfg_lines) + '\\n\\n\\n\\n')\n",
        "    if len(cfg.TRAIN.GPU_ID)==1:\n",
        "        torch.cuda.set_device(cfg.TRAIN.GPU_ID[0])\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    net = []   \n",
        "    \n",
        "    if cfg.TRAIN.STAGE=='all':\n",
        "        net = ENet(only_encode=False)\n",
        "        if cfg.TRAIN.PRETRAINED_ENCODER != '':\n",
        "            encoder_weight = torch.load(cfg.TRAIN.PRETRAINED_ENCODER)\n",
        "            del encoder_weight['classifier.bias']\n",
        "            del encoder_weight['classifier.weight']\n",
        "            # pdb.set_trace()\n",
        "            net.encoder.load_state_dict(encoder_weight)\n",
        "    elif cfg.TRAIN.STAGE =='encoder':\n",
        "        net = ENet(only_encode=True)\n",
        "\n",
        "    if len(cfg.TRAIN.GPU_ID)>1:\n",
        "        net = torch.nn.DataParallel(net, device_ids=cfg.TRAIN.GPU_ID).cuda()\n",
        "    else:\n",
        "        net=net.cuda()\n",
        "\n",
        "    net.train()\n",
        "    criterion = torch.nn.BCEWithLogitsLoss().cuda() # Binary Classification\n",
        "    optimizer = optim.Adam(net.parameters(), lr=cfg.TRAIN.LR, weight_decay=cfg.TRAIN.WEIGHT_DECAY)\n",
        "    scheduler = StepLR(optimizer, step_size=cfg.TRAIN.NUM_EPOCH_LR_DECAY, gamma=cfg.TRAIN.LR_DECAY)\n",
        "    _t = {'train time' : Timer(),'val time' : Timer()} \n",
        "    validate(val_loader, net, criterion, optimizer, -1, restore_transform)\n",
        "    for epoch in range(cfg.TRAIN.MAX_EPOCH):\n",
        "        _t['train time'].tic()\n",
        "        train(train_loader, net, criterion, optimizer, epoch)\n",
        "        _t['train time'].toc(average=False)\n",
        "        print('training time of one epoch: {:.2f}s'.format(_t['train time'].diff))\n",
        "        _t['val time'].tic()\n",
        "        validate(val_loader, net, criterion, optimizer, epoch, restore_transform)\n",
        "        _t['val time'].toc(average=False)\n",
        "        print('val time of one epoch: {:.2f}s'.format(_t['val time'].diff))\n",
        "\n",
        "\n",
        "def train(train_loader, net, criterion, optimizer, epoch):\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs = Variable(inputs).cuda()\n",
        "        labels = Variable(labels).cuda()\n",
        "   \n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels.unsqueeze(1).float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "def validate(val_loader, net, criterion, optimizer, epoch, restore):\n",
        "    net.eval()\n",
        "    criterion.cpu()\n",
        "    input_batches = []\n",
        "    output_batches = []\n",
        "    label_batches = []\n",
        "    iou_ = []\n",
        "    for vi, data in enumerate(val_loader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs = Variable(inputs, volatile=True).cuda()\n",
        "        labels = Variable(labels, volatile=True).cuda()\n",
        "        outputs = net(inputs)\n",
        "        #for binary classification\n",
        "        outputs[outputs>0.5] = 1\n",
        "        outputs[outputs<=0.5] = 0\n",
        "        #for multi-classification ???\n",
        "        iou_.append(calculate_iu_per_class([outputs.squeeze_(1).data.cpu().numpy()], [labels.data.cpu().numpy()],4))\n",
        "    mean_iu = np.mean(iou_, axis=0)   \n",
        "    print(\"Mean IoU for each class: \", mean_iu)\n",
        "    net.train()\n",
        "    criterion.cuda()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvHxB_MNVVf6",
        "outputId": "0b768048-464b-4e74-c170-5361bc99ad4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-440dfe10c50e>:96: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  inputs = Variable(inputs, volatile=True).cuda()\n",
            "<ipython-input-13-440dfe10c50e>:97: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  labels = Variable(labels, volatile=True).cuda()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean IoU for each class:  [9.66377840e-01 1.95824872e-14 1.00000000e+00 1.00000000e+00]\n",
            "training time of one epoch: 156.31s\n",
            "Mean IoU for each class:  [0.98245576 0.57168962 1.         1.        ]\n",
            "val time of one epoch: 40.58s\n",
            "training time of one epoch: 218.30s\n",
            "Mean IoU for each class:  [0.97852857 0.40895473 1.         1.        ]\n",
            "val time of one epoch: 40.69s\n",
            "training time of one epoch: 157.24s\n",
            "Mean IoU for each class:  [0.98417867 0.59060075 1.         1.        ]\n",
            "val time of one epoch: 319.71s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KZtsyPYlZFo2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}